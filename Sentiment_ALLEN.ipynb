{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled6.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ankit1khare/Sentiment_Analysis_is_fun_with_Allen_NLP/blob/master/Sentiment_ALLEN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "sMU2OoRg96Kk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Getting the resources\n",
        "!git clone https://github.com/mhagiwara/realworldnlp.git\n",
        "mv  ./realworldnlp/realworldnlp/predictors.py ./realworldnlp\n",
        "\n",
        "!wget https://nlp.stanford.edu/sentiment/trainDevTestTrees_PTB.\n",
        "!unzip trainDevTestTrees_PTB.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jnH3z8Ms9mAI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from typing import Dict\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from allennlp.data.dataset_readers.stanford_sentiment_tree_bank import \\\n",
        "    StanfordSentimentTreeBankDatasetReader\n",
        "from allennlp.data.iterators import BucketIterator\n",
        "from allennlp.data.vocabulary import Vocabulary\n",
        "from allennlp.models import Model\n",
        "from allennlp.modules.seq2vec_encoders import Seq2VecEncoder, PytorchSeq2VecWrapper\n",
        "from allennlp.modules.text_field_embedders import TextFieldEmbedder, BasicTextFieldEmbedder\n",
        "from allennlp.modules.token_embedders import Embedding\n",
        "from allennlp.nn.util import get_text_field_mask\n",
        "from allennlp.training.metrics import CategoricalAccuracy\n",
        "from allennlp.training.trainer import Trainer\n",
        "\n",
        "from realworldnlp.predictors import SentenceClassifierPredictor\n",
        "\n",
        "EMBEDDING_DIM = 128\n",
        "HIDDEN_DIM = 128\n",
        "\n",
        "# Model in AllenNLP represents a model that is trained.\n",
        "@Model.register(\"lstm_classify\") #use a name different from class for the model\n",
        "class LstmClassifier(Model):\n",
        "    def __init__(self,\n",
        "                 word_embeddings: TextFieldEmbedder,\n",
        "                 encoder: Seq2VecEncoder,\n",
        "                 vocab: Vocabulary) -> None:\n",
        "        super().__init__(vocab)\n",
        "        # We need the embeddings to convert word IDs to their vector representations\n",
        "        self.word_embeddings = word_embeddings\n",
        "\n",
        "        # Seq2VecEncoder is a neural network abstraction that takes a sequence of something\n",
        "        # (usually a sequence of embedded word vectors), processes it, and returns a single\n",
        "        # vector. Oftentimes this is an RNN-based architecture (e.g., LSTM or GRU), but\n",
        "        # AllenNLP also supports CNNs and other simple architectures (for example,\n",
        "        # just averaging over the input vectors).\n",
        "        self.encoder = encoder\n",
        "\n",
        "        # After converting a sequence of vectors to a single vector, we feed it into\n",
        "        # a fully-connected linear layer to reduce the dimension to the total number of labels.\n",
        "        self.hidden2tag = torch.nn.Linear(in_features=encoder.get_output_dim(),\n",
        "                                          out_features=vocab.get_vocab_size('labels'))\n",
        "        self.accuracy = CategoricalAccuracy()\n",
        "\n",
        "        # We use the cross entropy loss because this is a classification task.\n",
        "        # Note that PyTorch's CrossEntropyLoss combines softmax and log likelihood loss,\n",
        "        # which makes it unnecessary to add a separate softmax layer.\n",
        "        self.loss_function = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    # Instances are fed to forward after batching.\n",
        "    # Fields are passed through arguments with the same name.\n",
        "    def forward(self,\n",
        "                tokens: Dict[str, torch.Tensor],\n",
        "                label: torch.Tensor = None) -> torch.Tensor:\n",
        "        # In deep NLP, when sequences of tensors in different lengths are batched together,\n",
        "        # shorter sequences get padded with zeros to make them equal length.\n",
        "        # Masking is the process to ignore extra zeros added by padding\n",
        "        mask = get_text_field_mask(tokens)\n",
        "\n",
        "        # Forward pass\n",
        "        embeddings = self.word_embeddings(tokens)\n",
        "        encoder_out = self.encoder(embeddings, mask)\n",
        "        logits = self.hidden2tag(encoder_out)\n",
        "\n",
        "        # In AllenNLP, the output of forward() is a dictionary.\n",
        "        # Your output dictionary must contain a \"loss\" key for your model to be trained.\n",
        "        output = {\"logits\": logits}\n",
        "        if label is not None:\n",
        "            self.accuracy(logits, label)\n",
        "            output[\"loss\"] = self.loss_function(logits, label)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n",
        "        return {\"accuracy\": self.accuracy.get_metric(reset)}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Rcpuh1XLBpxy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# PATH = \"./model.pth\"\n",
        "def main():\n",
        "    reader = StanfordSentimentTreeBankDatasetReader()\n",
        "\n",
        "    train_dataset = reader.read('./trees/train.txt')\n",
        "    dev_dataset = reader.read('./trees/dev.txt')\n",
        "    \n",
        "    # You can optionally specify the minimum count of tokens/labels.\n",
        "    # `min_count={'tokens':3}` here means that any tokens that appear less than three times\n",
        "    # will be ignored and not included in the vocabulary.\n",
        "    vocab = Vocabulary.from_instances(train_dataset + dev_dataset,\n",
        "                                      min_count={'tokens': 3})\n",
        "\n",
        "    token_embedding = Embedding(num_embeddings=vocab.get_vocab_size('tokens'),\n",
        "                                embedding_dim=EMBEDDING_DIM)\n",
        "\n",
        "    # BasicTextFieldEmbedder takes a dict - we need an embedding just for tokens,\n",
        "    # not for labels, which are used as-is as the \"answer\" of the sentence classification\n",
        "    word_embeddings = BasicTextFieldEmbedder({\"tokens\": token_embedding})\n",
        "\n",
        "    lstm = PytorchSeq2VecWrapper(\n",
        "        torch.nn.LSTM(EMBEDDING_DIM, HIDDEN_DIM, batch_first=True))\n",
        "\n",
        "    model = LstmClassifier(word_embeddings, lstm, vocab)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "\n",
        "    iterator = BucketIterator(batch_size=32, sorting_keys=[(\"tokens\", \"num_tokens\")])\n",
        "\n",
        "    iterator.index_with(vocab)\n",
        "\n",
        "    trainer = Trainer(model=model,\n",
        "                      optimizer=optimizer,\n",
        "                      iterator=iterator,\n",
        "                      train_dataset=train_dataset,\n",
        "                      validation_dataset=dev_dataset,\n",
        "                      num_epochs=10)\n",
        "\n",
        "    trainer.train()\n",
        "#     torch.save(model, PATH)\n",
        "    tokens = ['This', 'is', 'the', 'worst', 'movie', 'ever', '!']\n",
        "# model = torch.load(PATH)\n",
        "# model.eval()\n",
        "\n",
        "    predictor = SentenceClassifierPredictor(model, dataset_reader=reader)\n",
        "    logits = predictor.predict(tokens)['logits']\n",
        "    label_id = np.argmax(logits)\n",
        "    \n",
        "    print(\"0 - very negative\\n 1 - negative\\n 2 - neutral\\n 3 - positive\\n 4 - very positive\\n\")\n",
        "    print(\"Sentiment level: {}\".format(model.vocab.get_token_from_index(label_id, 'labels')))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8v1P3m_1FO7k",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# tokens = ['This', 'is', 'the', 'best', 'movie', 'ever', '!']\n",
        "# # model = torch.load(PATH)\n",
        "# # model.eval()\n",
        "\n",
        "# # reader = StanfordSentimentTreeBankDatasetReader()\n",
        "# # train_dataset = reader.read('./trees/train.txt')\n",
        "# # dev_dataset = reader.read('./trees/dev.txt')\n",
        "  \n",
        "# predictor = SentenceClassifierPredictor(model, dataset_reader=reader)\n",
        "# logits = predictor.predict(tokens)['logits']\n",
        "# label_id = np.argmax(logits)\n",
        "\n",
        "# print(model.vocab.get_token_from_index(label_id, 'labels'))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RjKdTcBJ9pc6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2789
        },
        "outputId": "4672b232-4400-4325-b214-fcc71b098081"
      },
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0it [00:00, ?it/s]01/19/2019 18:25:26 - INFO - allennlp.data.dataset_readers.stanford_sentiment_tree_bank -   Reading instances from lines in file at: ./trees/train.txt\n",
            "8544it [00:01, 4334.71it/s]\n",
            "0it [00:00, ?it/s]01/19/2019 18:25:28 - INFO - allennlp.data.dataset_readers.stanford_sentiment_tree_bank -   Reading instances from lines in file at: ./trees/dev.txt\n",
            "1101it [00:00, 4616.91it/s]\n",
            "01/19/2019 18:25:28 - INFO - allennlp.data.vocabulary -   Fitting token dictionary from dataset.\n",
            "100%|██████████| 9645/9645 [00:00<00:00, 48429.53it/s]\n",
            "01/19/2019 18:25:28 - WARNING - allennlp.training.trainer -   You provided a validation dataset but patience was set to None, meaning that early stopping is disabled\n",
            "01/19/2019 18:25:28 - INFO - allennlp.training.trainer -   Beginning training.\n",
            "01/19/2019 18:25:28 - INFO - allennlp.training.trainer -   Epoch 0/9\n",
            "01/19/2019 18:25:28 - INFO - allennlp.training.trainer -   Peak CPU memory usage MB: 517.616\n",
            "01/19/2019 18:25:28 - INFO - allennlp.training.trainer -   GPU 0 memory usage MB: 11\n",
            "01/19/2019 18:25:28 - INFO - allennlp.training.trainer -   Training\n",
            "accuracy: 0.2577, loss: 1.5797 ||: 100%|██████████| 267/267 [00:11<00:00, 23.68it/s]\n",
            "01/19/2019 18:25:40 - INFO - allennlp.training.trainer -   Validating\n",
            "accuracy: 0.2607, loss: 1.5733 ||: 100%|██████████| 35/35 [00:00<00:00, 74.19it/s]\n",
            "01/19/2019 18:25:40 - INFO - allennlp.training.trainer -                       Training |  Validation\n",
            "01/19/2019 18:25:40 - INFO - allennlp.training.trainer -   cpu_memory_MB   |   517.616  |       N/A\n",
            "01/19/2019 18:25:40 - INFO - allennlp.training.trainer -   accuracy        |     0.258  |     0.261\n",
            "01/19/2019 18:25:40 - INFO - allennlp.training.trainer -   loss            |     1.580  |     1.573\n",
            "01/19/2019 18:25:40 - INFO - allennlp.training.trainer -   gpu_0_memory_MB |    11.000  |       N/A\n",
            "01/19/2019 18:25:40 - INFO - allennlp.training.trainer -   Epoch duration: 00:00:11\n",
            "01/19/2019 18:25:40 - INFO - allennlp.training.trainer -   Estimated training time remaining: 0:01:46\n",
            "01/19/2019 18:25:40 - INFO - allennlp.training.trainer -   Epoch 1/9\n",
            "01/19/2019 18:25:40 - INFO - allennlp.training.trainer -   Peak CPU memory usage MB: 517.616\n",
            "01/19/2019 18:25:40 - INFO - allennlp.training.trainer -   GPU 0 memory usage MB: 11\n",
            "01/19/2019 18:25:40 - INFO - allennlp.training.trainer -   Training\n",
            "accuracy: 0.2707, loss: 1.5657 ||: 100%|██████████| 267/267 [00:11<00:00, 26.44it/s]\n",
            "01/19/2019 18:25:51 - INFO - allennlp.training.trainer -   Validating\n",
            "accuracy: 0.2534, loss: 1.5711 ||: 100%|██████████| 35/35 [00:00<00:00, 83.46it/s]\n",
            "01/19/2019 18:25:52 - INFO - allennlp.training.trainer -                       Training |  Validation\n",
            "01/19/2019 18:25:52 - INFO - allennlp.training.trainer -   cpu_memory_MB   |   517.616  |       N/A\n",
            "01/19/2019 18:25:52 - INFO - allennlp.training.trainer -   accuracy        |     0.271  |     0.253\n",
            "01/19/2019 18:25:52 - INFO - allennlp.training.trainer -   loss            |     1.566  |     1.571\n",
            "01/19/2019 18:25:52 - INFO - allennlp.training.trainer -   gpu_0_memory_MB |    11.000  |       N/A\n",
            "01/19/2019 18:25:52 - INFO - allennlp.training.trainer -   Epoch duration: 00:00:11\n",
            "01/19/2019 18:25:52 - INFO - allennlp.training.trainer -   Estimated training time remaining: 0:01:33\n",
            "01/19/2019 18:25:52 - INFO - allennlp.training.trainer -   Epoch 2/9\n",
            "01/19/2019 18:25:52 - INFO - allennlp.training.trainer -   Peak CPU memory usage MB: 517.616\n",
            "01/19/2019 18:25:52 - INFO - allennlp.training.trainer -   GPU 0 memory usage MB: 11\n",
            "01/19/2019 18:25:52 - INFO - allennlp.training.trainer -   Training\n",
            "accuracy: 0.2774, loss: 1.5592 ||: 100%|██████████| 267/267 [00:10<00:00, 24.11it/s]\n",
            "01/19/2019 18:26:03 - INFO - allennlp.training.trainer -   Validating\n",
            "accuracy: 0.2561, loss: 1.5685 ||: 100%|██████████| 35/35 [00:00<00:00, 83.40it/s]\n",
            "01/19/2019 18:26:03 - INFO - allennlp.training.trainer -                       Training |  Validation\n",
            "01/19/2019 18:26:03 - INFO - allennlp.training.trainer -   cpu_memory_MB   |   517.616  |       N/A\n",
            "01/19/2019 18:26:03 - INFO - allennlp.training.trainer -   accuracy        |     0.277  |     0.256\n",
            "01/19/2019 18:26:03 - INFO - allennlp.training.trainer -   loss            |     1.559  |     1.569\n",
            "01/19/2019 18:26:03 - INFO - allennlp.training.trainer -   gpu_0_memory_MB |    11.000  |       N/A\n",
            "01/19/2019 18:26:03 - INFO - allennlp.training.trainer -   Epoch duration: 00:00:11\n",
            "01/19/2019 18:26:03 - INFO - allennlp.training.trainer -   Estimated training time remaining: 0:01:21\n",
            "01/19/2019 18:26:03 - INFO - allennlp.training.trainer -   Epoch 3/9\n",
            "01/19/2019 18:26:03 - INFO - allennlp.training.trainer -   Peak CPU memory usage MB: 517.616\n",
            "01/19/2019 18:26:03 - INFO - allennlp.training.trainer -   GPU 0 memory usage MB: 11\n",
            "01/19/2019 18:26:03 - INFO - allennlp.training.trainer -   Training\n",
            "accuracy: 0.2834, loss: 1.5351 ||: 100%|██████████| 267/267 [00:10<00:00, 26.27it/s]\n",
            "01/19/2019 18:26:14 - INFO - allennlp.training.trainer -   Validating\n",
            "accuracy: 0.2589, loss: 1.5558 ||: 100%|██████████| 35/35 [00:00<00:00, 80.18it/s]\n",
            "01/19/2019 18:26:14 - INFO - allennlp.training.trainer -                       Training |  Validation\n",
            "01/19/2019 18:26:14 - INFO - allennlp.training.trainer -   cpu_memory_MB   |   517.616  |       N/A\n",
            "01/19/2019 18:26:14 - INFO - allennlp.training.trainer -   accuracy        |     0.283  |     0.259\n",
            "01/19/2019 18:26:14 - INFO - allennlp.training.trainer -   loss            |     1.535  |     1.556\n",
            "01/19/2019 18:26:14 - INFO - allennlp.training.trainer -   gpu_0_memory_MB |    11.000  |       N/A\n",
            "01/19/2019 18:26:14 - INFO - allennlp.training.trainer -   Epoch duration: 00:00:11\n",
            "01/19/2019 18:26:14 - INFO - allennlp.training.trainer -   Estimated training time remaining: 0:01:09\n",
            "01/19/2019 18:26:14 - INFO - allennlp.training.trainer -   Epoch 4/9\n",
            "01/19/2019 18:26:14 - INFO - allennlp.training.trainer -   Peak CPU memory usage MB: 517.616\n",
            "01/19/2019 18:26:15 - INFO - allennlp.training.trainer -   GPU 0 memory usage MB: 11\n",
            "01/19/2019 18:26:15 - INFO - allennlp.training.trainer -   Training\n",
            "accuracy: 0.3322, loss: 1.4681 ||: 100%|██████████| 267/267 [00:11<00:00, 24.13it/s]\n",
            "01/19/2019 18:26:26 - INFO - allennlp.training.trainer -   Validating\n",
            "accuracy: 0.3297, loss: 1.5278 ||: 100%|██████████| 35/35 [00:00<00:00, 79.64it/s]\n",
            "01/19/2019 18:26:26 - INFO - allennlp.training.trainer -                       Training |  Validation\n",
            "01/19/2019 18:26:26 - INFO - allennlp.training.trainer -   cpu_memory_MB   |   517.616  |       N/A\n",
            "01/19/2019 18:26:26 - INFO - allennlp.training.trainer -   accuracy        |     0.332  |     0.330\n",
            "01/19/2019 18:26:26 - INFO - allennlp.training.trainer -   loss            |     1.468  |     1.528\n",
            "01/19/2019 18:26:26 - INFO - allennlp.training.trainer -   gpu_0_memory_MB |    11.000  |       N/A\n",
            "01/19/2019 18:26:26 - INFO - allennlp.training.trainer -   Epoch duration: 00:00:11\n",
            "01/19/2019 18:26:26 - INFO - allennlp.training.trainer -   Estimated training time remaining: 0:00:57\n",
            "01/19/2019 18:26:26 - INFO - allennlp.training.trainer -   Epoch 5/9\n",
            "01/19/2019 18:26:26 - INFO - allennlp.training.trainer -   Peak CPU memory usage MB: 517.616\n",
            "01/19/2019 18:26:26 - INFO - allennlp.training.trainer -   GPU 0 memory usage MB: 11\n",
            "01/19/2019 18:26:26 - INFO - allennlp.training.trainer -   Training\n",
            "accuracy: 0.4402, loss: 1.3304 ||: 100%|██████████| 267/267 [00:11<00:00, 25.77it/s]\n",
            "01/19/2019 18:26:37 - INFO - allennlp.training.trainer -   Validating\n",
            "accuracy: 0.3615, loss: 1.4750 ||: 100%|██████████| 35/35 [00:00<00:00, 79.69it/s]\n",
            "01/19/2019 18:26:38 - INFO - allennlp.training.trainer -                       Training |  Validation\n",
            "01/19/2019 18:26:38 - INFO - allennlp.training.trainer -   cpu_memory_MB   |   517.616  |       N/A\n",
            "01/19/2019 18:26:38 - INFO - allennlp.training.trainer -   accuracy        |     0.440  |     0.361\n",
            "01/19/2019 18:26:38 - INFO - allennlp.training.trainer -   loss            |     1.330  |     1.475\n",
            "01/19/2019 18:26:38 - INFO - allennlp.training.trainer -   gpu_0_memory_MB |    11.000  |       N/A\n",
            "01/19/2019 18:26:38 - INFO - allennlp.training.trainer -   Epoch duration: 00:00:11\n",
            "01/19/2019 18:26:38 - INFO - allennlp.training.trainer -   Estimated training time remaining: 0:00:46\n",
            "01/19/2019 18:26:38 - INFO - allennlp.training.trainer -   Epoch 6/9\n",
            "01/19/2019 18:26:38 - INFO - allennlp.training.trainer -   Peak CPU memory usage MB: 517.616\n",
            "01/19/2019 18:26:38 - INFO - allennlp.training.trainer -   GPU 0 memory usage MB: 11\n",
            "01/19/2019 18:26:38 - INFO - allennlp.training.trainer -   Training\n",
            "accuracy: 0.5202, loss: 1.1803 ||: 100%|██████████| 267/267 [00:11<00:00, 23.04it/s]\n",
            "01/19/2019 18:26:49 - INFO - allennlp.training.trainer -   Validating\n",
            "accuracy: 0.3724, loss: 1.4434 ||: 100%|██████████| 35/35 [00:00<00:00, 76.14it/s]\n",
            "01/19/2019 18:26:49 - INFO - allennlp.training.trainer -                       Training |  Validation\n",
            "01/19/2019 18:26:49 - INFO - allennlp.training.trainer -   cpu_memory_MB   |   517.616  |       N/A\n",
            "01/19/2019 18:26:49 - INFO - allennlp.training.trainer -   accuracy        |     0.520  |     0.372\n",
            "01/19/2019 18:26:49 - INFO - allennlp.training.trainer -   loss            |     1.180  |     1.443\n",
            "01/19/2019 18:26:49 - INFO - allennlp.training.trainer -   gpu_0_memory_MB |    11.000  |       N/A\n",
            "01/19/2019 18:26:49 - INFO - allennlp.training.trainer -   Epoch duration: 00:00:11\n",
            "01/19/2019 18:26:49 - INFO - allennlp.training.trainer -   Estimated training time remaining: 0:00:34\n",
            "01/19/2019 18:26:49 - INFO - allennlp.training.trainer -   Epoch 7/9\n",
            "01/19/2019 18:26:49 - INFO - allennlp.training.trainer -   Peak CPU memory usage MB: 517.616\n",
            "01/19/2019 18:26:49 - INFO - allennlp.training.trainer -   GPU 0 memory usage MB: 11\n",
            "01/19/2019 18:26:49 - INFO - allennlp.training.trainer -   Training\n",
            "accuracy: 0.5764, loss: 1.0506 ||: 100%|██████████| 267/267 [00:11<00:00, 25.48it/s]\n",
            "01/19/2019 18:27:00 - INFO - allennlp.training.trainer -   Validating\n",
            "accuracy: 0.3769, loss: 1.5056 ||: 100%|██████████| 35/35 [00:00<00:00, 79.39it/s]\n",
            "01/19/2019 18:27:01 - INFO - allennlp.training.trainer -                       Training |  Validation\n",
            "01/19/2019 18:27:01 - INFO - allennlp.training.trainer -   cpu_memory_MB   |   517.616  |       N/A\n",
            "01/19/2019 18:27:01 - INFO - allennlp.training.trainer -   accuracy        |     0.576  |     0.377\n",
            "01/19/2019 18:27:01 - INFO - allennlp.training.trainer -   loss            |     1.051  |     1.506\n",
            "01/19/2019 18:27:01 - INFO - allennlp.training.trainer -   gpu_0_memory_MB |    11.000  |       N/A\n",
            "01/19/2019 18:27:01 - INFO - allennlp.training.trainer -   Epoch duration: 00:00:11\n",
            "01/19/2019 18:27:01 - INFO - allennlp.training.trainer -   Estimated training time remaining: 0:00:23\n",
            "01/19/2019 18:27:01 - INFO - allennlp.training.trainer -   Epoch 8/9\n",
            "01/19/2019 18:27:01 - INFO - allennlp.training.trainer -   Peak CPU memory usage MB: 517.616\n",
            "01/19/2019 18:27:01 - INFO - allennlp.training.trainer -   GPU 0 memory usage MB: 11\n",
            "01/19/2019 18:27:01 - INFO - allennlp.training.trainer -   Training\n",
            "accuracy: 0.6174, loss: 0.9472 ||: 100%|██████████| 267/267 [00:10<00:00, 24.29it/s]\n",
            "01/19/2019 18:27:12 - INFO - allennlp.training.trainer -   Validating\n",
            "accuracy: 0.3797, loss: 1.5537 ||: 100%|██████████| 35/35 [00:00<00:00, 77.16it/s]\n",
            "01/19/2019 18:27:12 - INFO - allennlp.training.trainer -                       Training |  Validation\n",
            "01/19/2019 18:27:12 - INFO - allennlp.training.trainer -   cpu_memory_MB   |   517.616  |       N/A\n",
            "01/19/2019 18:27:12 - INFO - allennlp.training.trainer -   accuracy        |     0.617  |     0.380\n",
            "01/19/2019 18:27:12 - INFO - allennlp.training.trainer -   loss            |     0.947  |     1.554\n",
            "01/19/2019 18:27:12 - INFO - allennlp.training.trainer -   gpu_0_memory_MB |    11.000  |       N/A\n",
            "01/19/2019 18:27:12 - INFO - allennlp.training.trainer -   Epoch duration: 00:00:11\n",
            "01/19/2019 18:27:12 - INFO - allennlp.training.trainer -   Estimated training time remaining: 0:00:11\n",
            "01/19/2019 18:27:12 - INFO - allennlp.training.trainer -   Epoch 9/9\n",
            "01/19/2019 18:27:12 - INFO - allennlp.training.trainer -   Peak CPU memory usage MB: 517.616\n",
            "01/19/2019 18:27:12 - INFO - allennlp.training.trainer -   GPU 0 memory usage MB: 11\n",
            "01/19/2019 18:27:12 - INFO - allennlp.training.trainer -   Training\n",
            "accuracy: 0.6498, loss: 0.8635 ||: 100%|██████████| 267/267 [00:11<00:00, 24.26it/s]\n",
            "01/19/2019 18:27:23 - INFO - allennlp.training.trainer -   Validating\n",
            "accuracy: 0.3778, loss: 1.6837 ||: 100%|██████████| 35/35 [00:00<00:00, 77.23it/s]\n",
            "01/19/2019 18:27:24 - INFO - allennlp.training.trainer -                       Training |  Validation\n",
            "01/19/2019 18:27:24 - INFO - allennlp.training.trainer -   cpu_memory_MB   |   517.616  |       N/A\n",
            "01/19/2019 18:27:24 - INFO - allennlp.training.trainer -   accuracy        |     0.650  |     0.378\n",
            "01/19/2019 18:27:24 - INFO - allennlp.training.trainer -   loss            |     0.863  |     1.684\n",
            "01/19/2019 18:27:24 - INFO - allennlp.training.trainer -   gpu_0_memory_MB |    11.000  |       N/A\n",
            "01/19/2019 18:27:24 - INFO - allennlp.training.trainer -   Epoch duration: 00:00:11\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0 - very negative\n",
            " 1 - negative\n",
            " 2 - neutral\n",
            " 3 - positive\n",
            " 4 - very positive\n",
            "\n",
            "Sentiment level: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "A5Kcawz3B9fA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}