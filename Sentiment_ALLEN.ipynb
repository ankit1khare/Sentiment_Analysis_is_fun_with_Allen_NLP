{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled6.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ankit1khare/Sentiment_Analysis_is_fun_with_Allen_NLP/blob/master/Sentiment_ALLEN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "sMU2OoRg96Kk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "01cf0da3-054b-4207-cf13-d843e66bf29c"
      },
      "cell_type": "code",
      "source": [
        "#Getting the resources\n",
        "!git clone https://github.com/mhagiwara/realworldnlp.git\n",
        "\n",
        "!wget https://nlp.stanford.edu/sentiment/trainDevTestTrees_PTB.zip\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'realworldnlp' already exists and is not an empty directory.\n",
            "--2019-01-20 08:30:13--  https://nlp.stanford.edu/sentiment/trainDevTestTrees_PTB.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 789539 (771K) [application/zip]\n",
            "Saving to: ‘trainDevTestTrees_PTB.zip’\n",
            "\n",
            "trainDevTestTrees_P 100%[===================>] 771.03K  3.44MB/s    in 0.2s    \n",
            "\n",
            "2019-01-20 08:30:13 (3.44 MB/s) - ‘trainDevTestTrees_PTB.zip’ saved [789539/789539]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ApdEWoDgYiEQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3435
        },
        "outputId": "71eab621-73bd-4a5e-b9f1-ce58baee6966"
      },
      "cell_type": "code",
      "source": [
        "!pip install allennlp"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting allennlp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d6/37/e2418f74df007e9516013828205462021e330c27aed8ffc64c6fca44c7d6/allennlp-0.8.1-py3-none-any.whl (5.5MB)\n",
            "\u001b[K    100% |████████████████████████████████| 5.5MB 1.1MB/s \n",
            "\u001b[?25hCollecting responses>=0.7 (from allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/2f/d8/a77cbd4cb8366ad0e275f2c642b50b401da22a2f5714e003e499fddca106/responses-0.10.5-py2.py3-none-any.whl\n",
            "Requirement already satisfied: msgpack<0.6.0,>=0.5.6 in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.5.6)\n",
            "Requirement already satisfied: cffi==1.11.5 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.11.5)\n",
            "Collecting torch>=0.4.1 (from allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/60/66415660aa46b23b5e1b72bc762e816736ce8d7260213e22365af51e8f9c/torch-1.0.0-cp36-cp36m-manylinux1_x86_64.whl (591.8MB)\n",
            "\u001b[K    100% |████████████████████████████████| 591.8MB 27kB/s \n",
            "tcmalloc: large alloc 1073750016 bytes == 0x626ca000 @  0x7f1ca4c1c2a4 0x591a07 0x5b5d56 0x502e9a 0x506859 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x504c28 0x502540 0x502f3d 0x507641\n",
            "\u001b[?25hCollecting moto==1.3.4 (from allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ee/8f/7b36e81ff067d0e7bf90f7210b351c0cfe6657f79fa4dcb0cb4787462e05/moto-1.3.4-py2.py3-none-any.whl (548kB)\n",
            "\u001b[K    100% |████████████████████████████████| 552kB 8.3MB/s \n",
            "\u001b[?25hCollecting jsonnet==0.10.0; sys_platform != \"win32\" (from allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/83/d49904ee98dd4fbba6a003938e30e76251951c4bdb49628b4f92e5009a42/jsonnet-0.10.0.tar.gz (124kB)\n",
            "\u001b[K    100% |████████████████████████████████| 133kB 33.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.10.1)\n",
            "Collecting pytorch-pretrained-bert==0.3.0 (from allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/26/85/de4dd7e018a197280752881adf7b4142886f20155145f641f7c803c0018a/pytorch_pretrained_bert-0.3.0-py3-none-any.whl\n",
            "Collecting overrides (from allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/de/55/3100c6d14c1ed177492fcf8f07c4a7d2d6c996c0a7fc6a9a0a41308e7eec/overrides-1.9.tar.gz\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.1.0)\n",
            "Collecting ftfy (from allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/d1/b25441b6b8f22dfd9547711268562fe651b74cf66288cce632884025e085/ftfy-5.5.0.tar.gz (56kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 23.8MB/s \n",
            "\u001b[?25hCollecting awscli>=1.11.91 (from allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/41/b5/4e48f49bcf38d10d027b419b9be0a0ce98a370a5f75e1a2e0640073e1a5b/awscli-1.16.92-py2.py3-none-any.whl (1.4MB)\n",
            "\u001b[K    100% |████████████████████████████████| 1.4MB 6.7MB/s \n",
            "\u001b[?25hCollecting matplotlib==2.2.3 (from allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/59/f235ab21bbe7b7c6570c4abf17ffb893071f4fa3b9cf557b09b60359ad9a/matplotlib-2.2.3-cp36-cp36m-manylinux1_x86_64.whl (12.6MB)\n",
            "\u001b[K    100% |████████████████████████████████| 12.6MB 1.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: sqlparse==0.2.4 in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.2.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.2.5)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.9.78)\n",
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.0.23)\n",
            "Requirement already satisfied: tqdm>=4.19 in /usr/local/lib/python3.6/dist-packages (from allennlp) (4.28.1)\n",
            "Collecting tensorboardX==1.2 (from allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c5/22/43f4f0318f7c68a1000dbb700a353b745584bc2397437832d15ba69ea5f1/tensorboardX-1.2-py2.py3-none-any.whl (44kB)\n",
            "\u001b[K    100% |████████████████████████████████| 51kB 23.7MB/s \n",
            "\u001b[?25hCollecting pytz==2017.3 (from allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/7f/e7d1acbd433b929168a4fb4182a2ff3c33653717195a26c1de099ad1ef29/pytz-2017.3-py2.py3-none-any.whl (511kB)\n",
            "\u001b[K    100% |████████████████████████████████| 512kB 3.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: editdistance in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.5.2)\n",
            "Collecting numpydoc==0.8.0 (from allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/95/a8/b4706a6270f0475541c5c1ee3373c7a3b793936ec1f517f1a1dab4f896c0/numpydoc-0.8.0.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.14.6)\n",
            "Collecting gevent==1.3.6 (from allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/43/3d/a19fece28ba1b5133cf74bd22a229d77b4d9cc4b24aa8f263cca2845c555/gevent-1.3.6-cp36-cp36m-manylinux1_x86_64.whl (4.5MB)\n",
            "\u001b[K    100% |████████████████████████████████| 4.5MB 3.4MB/s \n",
            "\u001b[?25hCollecting conllu==0.11 (from allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/d4/2c/856344d9b69baf5b374c395b4286626181a80f0c2b2f704914d18a1cea47/conllu-0.11-py2.py3-none-any.whl\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.8.0)\n",
            "Requirement already satisfied: flask==1.0.2 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.0.2)\n",
            "Collecting parsimonious==0.8.0 (from allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/4a/89/32c55944cd30dff856f16859ee325b13c83c260d0c56c0eed511e8063c87/parsimonious-0.8.0.tar.gz\n",
            "Requirement already satisfied: spacy<2.1,>=2.0 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.0.18)\n",
            "Collecting flask-cors==3.0.7 (from allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/65/cb/683f71ff8daa3aea0a5cbb276074de39f9ab66d3fbb8ad5efb5bb83e90d2/Flask_Cors-3.0.7-py2.py3-none-any.whl\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.20.2)\n",
            "Collecting flaky (from allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/02/42/cca66659a786567c8af98587d66d75e7d2b6e65662f8daab75db708ac35b/flaky-3.5.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied: requests>=2.18 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.18.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from responses>=0.7->allennlp) (1.11.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi==1.11.5->allennlp) (2.19)\n",
            "Requirement already satisfied: Jinja2>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from moto==1.3.4->allennlp) (2.10)\n",
            "Collecting python-jose<3.0.0 (from moto==1.3.4->allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/bf/5c/5fa238c0c5b0656994b52721dd8b1d7bf52ebd8786518dde794f44de86b6/python_jose-2.0.2-py2.py3-none-any.whl\n",
            "Collecting docker>=2.5.1 (from moto==1.3.4->allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/3c/b610f22b170b0f8fe4d8f78974878e116562389f666f99e6549567eb9d87/docker-3.7.0-py2.py3-none-any.whl (133kB)\n",
            "\u001b[K    100% |████████████████████████████████| 143kB 31.4MB/s \n",
            "\u001b[?25hCollecting jsondiff==1.1.1 (from moto==1.3.4->allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/bd/5f/13e28a2f9abeda2ffb3f44f2f809b01b52bc02cdb63816e05b8c9cbbdfc5/jsondiff-1.1.1.tar.gz\n",
            "Collecting aws-xray-sdk<0.96,>=0.93 (from moto==1.3.4->allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a4/a5/da7887285564f9e0ae5cd25a453cca36e2cd43d8ccc9effde260b4d80904/aws_xray_sdk-0.95-py2.py3-none-any.whl (52kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 25.5MB/s \n",
            "\u001b[?25hCollecting xmltodict (from moto==1.3.4->allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/42/a9/7e99652c6bc619d19d58cdd8c47560730eb5825d43a7e25db2e1d776ceb7/xmltodict-0.11.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from moto==1.3.4->allennlp) (2.5.3)\n",
            "Collecting mock (from moto==1.3.4->allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e6/35/f187bdf23be87092bd0f1200d43d23076cee4d0dec109f195173fd3ebc79/mock-2.0.0-py2.py3-none-any.whl (56kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 20.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: werkzeug in /usr/local/lib/python3.6/dist-packages (from moto==1.3.4->allennlp) (0.14.1)\n",
            "Collecting pyaml (from moto==1.3.4->allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/c5/e1/1523fb1dab744e2c6b1f02446f2139a78726c18c062a8ddd53875abb20f8/pyaml-18.11.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: boto>=2.36.0 in /usr/local/lib/python3.6/dist-packages (from moto==1.3.4->allennlp) (2.49.0)\n",
            "Collecting cookies (from moto==1.3.4->allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6a/60/557f84aa2db629e5124aa05408b975b1b5d0e1cec16cde0bfa06aae097d3/cookies-2.2.1-py2.py3-none-any.whl (44kB)\n",
            "\u001b[K    100% |████████████████████████████████| 51kB 23.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: botocore>=1.9.16 in /usr/local/lib/python3.6/dist-packages (from moto==1.3.4->allennlp) (1.12.78)\n",
            "Collecting cryptography>=2.0.0 (from moto==1.3.4->allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/c7/99b33c53cf3f20a97a4c4bfd3ab66dcc93d99da0a97cc9597aa36ae6bb62/cryptography-2.4.2-cp34-abi3-manylinux1_x86_64.whl (2.1MB)\n",
            "\u001b[K    100% |████████████████████████████████| 2.1MB 6.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (18.2.0)\n",
            "Requirement already satisfied: pluggy>=0.7 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (0.8.1)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (1.7.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (5.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (40.6.3)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (1.2.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy->allennlp) (0.1.7)\n",
            "Requirement already satisfied: docutils>=0.10 in /usr/local/lib/python3.6/dist-packages (from awscli>=1.11.91->allennlp) (0.14)\n",
            "Collecting rsa<=3.5.0,>=3.1.2 (from awscli>=1.11.91->allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e1/ae/baedc9cb175552e95f3395c43055a6a5e125ae4d48a1d7a924baca83e92e/rsa-3.4.2-py2.py3-none-any.whl (46kB)\n",
            "\u001b[K    100% |████████████████████████████████| 51kB 22.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML<=3.13,>=3.10 in /usr/local/lib/python3.6/dist-packages (from awscli>=1.11.91->allennlp) (3.13)\n",
            "Requirement already satisfied: s3transfer<0.2.0,>=0.1.12 in /usr/local/lib/python3.6/dist-packages (from awscli>=1.11.91->allennlp) (0.1.13)\n",
            "Collecting colorama<=0.3.9,>=0.2.5 (from awscli>=1.11.91->allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/db/c8/7dcf9dbcb22429512708fe3a547f8b6101c0d02137acbd892505aee57adf/colorama-0.3.9-py2.py3-none-any.whl\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib==2.2.3->allennlp) (2.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib==2.2.3->allennlp) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib==2.2.3->allennlp) (1.0.1)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->allennlp) (0.9.3)\n",
            "Requirement already satisfied: protobuf>=0.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorboardX==1.2->allennlp) (3.6.1)\n",
            "Requirement already satisfied: sphinx>=1.2.3 in /usr/local/lib/python3.6/dist-packages (from numpydoc==0.8.0->allennlp) (1.8.3)\n",
            "Requirement already satisfied: greenlet>=0.4.14; platform_python_implementation == \"CPython\" in /usr/local/lib/python3.6/dist-packages (from gevent==1.3.6->allennlp) (0.4.15)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from flask==1.0.2->allennlp) (1.1.0)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from flask==1.0.2->allennlp) (7.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.1,>=2.0->allennlp) (1.0.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.1,>=2.0->allennlp) (2.0.2)\n",
            "Requirement already satisfied: ujson>=1.35 in /usr/local/lib/python3.6/dist-packages (from spacy<2.1,>=2.0->allennlp) (1.35)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy<2.1,>=2.0->allennlp) (2.0.1)\n",
            "Requirement already satisfied: dill<0.3,>=0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.1,>=2.0->allennlp) (0.2.8.2)\n",
            "Requirement already satisfied: thinc<6.13.0,>=6.12.1 in /usr/local/lib/python3.6/dist-packages (from spacy<2.1,>=2.0->allennlp) (6.12.1)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy<2.1,>=2.0->allennlp) (0.9.6)\n",
            "Requirement already satisfied: regex==2018.01.10 in /usr/local/lib/python3.6/dist-packages (from spacy<2.1,>=2.0->allennlp) (2018.1.10)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (1.22)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (2018.11.29)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (3.0.4)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (2.6)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.7.3->moto==1.3.4->allennlp) (1.1.0)\n",
            "Collecting ecdsa<1.0 (from python-jose<3.0.0->moto==1.3.4->allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/63/f4/73669d51825516ce8c43b816c0a6b64cd6eb71d08b99820c00792cb42222/ecdsa-0.13-py2.py3-none-any.whl (86kB)\n",
            "\u001b[K    100% |████████████████████████████████| 92kB 29.8MB/s \n",
            "\u001b[?25hCollecting pycryptodome<4.0.0,>=3.3.1 (from python-jose<3.0.0->moto==1.3.4->allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ab/bc/589fbae48c0969719f28b862d979f8e9d4d1e08bb7f68ad7a495b1ba2d54/pycryptodome-3.7.2-cp36-cp36m-manylinux1_x86_64.whl (7.5MB)\n",
            "\u001b[K    100% |████████████████████████████████| 7.5MB 2.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: future<1.0 in /usr/local/lib/python3.6/dist-packages (from python-jose<3.0.0->moto==1.3.4->allennlp) (0.16.0)\n",
            "Collecting docker-pycreds>=0.4.0 (from docker>=2.5.1->moto==1.3.4->allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl\n",
            "Collecting websocket-client>=0.32.0 (from docker>=2.5.1->moto==1.3.4->allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/26/2d/f749a5c82f6192d77ed061a38e02001afcba55fe8477336d26a950ab17ce/websocket_client-0.54.0-py2.py3-none-any.whl (200kB)\n",
            "\u001b[K    100% |████████████████████████████████| 204kB 26.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt in /usr/local/lib/python3.6/dist-packages (from aws-xray-sdk<0.96,>=0.93->moto==1.3.4->allennlp) (1.11.0)\n",
            "Collecting jsonpickle (from aws-xray-sdk<0.96,>=0.93->moto==1.3.4->allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/ca/ce/97404d5aeb58e6155c216825c81b50f6eca8a5345c582317ae48391878f8/jsonpickle-1.0-py2.py3-none-any.whl\n",
            "Collecting pbr>=0.11 (from mock->moto==1.3.4->allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f3/04/fddc1c2dd75b256eda4d360024692231a2c19a0c61ad7f4a162407c1ab58/pbr-5.1.1-py2.py3-none-any.whl (106kB)\n",
            "\u001b[K    100% |████████████████████████████████| 112kB 29.9MB/s \n",
            "\u001b[?25hCollecting asn1crypto>=0.21.0 (from cryptography>=2.0.0->moto==1.3.4->allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/cd/35485615f45f30a510576f1a56d1e0a7ad7bd8ab5ed7cdc600ef7cd06222/asn1crypto-0.24.0-py2.py3-none-any.whl (101kB)\n",
            "\u001b[K    100% |████████████████████████████████| 102kB 22.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<=3.5.0,>=3.1.2->awscli>=1.11.91->allennlp) (0.4.5)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.2.3->numpydoc==0.8.0->allennlp) (1.2.1)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.2.3->numpydoc==0.8.0->allennlp) (0.7.12)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.2.3->numpydoc==0.8.0->allennlp) (1.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.2.3->numpydoc==0.8.0->allennlp) (18.0)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.2.3->numpydoc==0.8.0->allennlp) (2.1.3)\n",
            "Requirement already satisfied: sphinxcontrib-websupport in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.2.3->numpydoc==0.8.0->allennlp) (1.1.0)\n",
            "Requirement already satisfied: babel!=2.0,>=1.3 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.2.3->numpydoc==0.8.0->allennlp) (2.6.0)\n",
            "Requirement already satisfied: cytoolz<0.10,>=0.9.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy<2.1,>=2.0->allennlp) (0.9.0.1)\n",
            "Requirement already satisfied: msgpack-numpy<0.4.4 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy<2.1,>=2.0->allennlp) (0.4.3.2)\n",
            "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from cytoolz<0.10,>=0.9.0->thinc<6.13.0,>=6.12.1->spacy<2.1,>=2.0->allennlp) (0.9.0)\n",
            "Building wheels for collected packages: jsonnet, overrides, ftfy, numpydoc, parsimonious, jsondiff\n",
            "  Running setup.py bdist_wheel for jsonnet ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/a0/aa/f0/b4ab8854cf00f922a87787425cfbb789aac01ab2c2cd1b4ca4\n",
            "  Running setup.py bdist_wheel for overrides ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/8d/52/86/e5a83b1797e7d263b458d2334edd2704c78508b3eea9323718\n",
            "  Running setup.py bdist_wheel for ftfy ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/e6/42/b1/0d7f9445a9df8920548ab9bb953d6587067c34bc47e5be9620\n",
            "  Running setup.py bdist_wheel for numpydoc ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/ea/55/7f/3e25d754760ccd62d6796e5b2cfe25629346f52ea00753d549\n",
            "  Running setup.py bdist_wheel for parsimonious ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/bb/51/82/ae9b22a790f11e7be918939d01aa397c545ebb3723453c5fb4\n",
            "  Running setup.py bdist_wheel for jsondiff ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/68/08/07/69d839606fb7fdc778fa86476abc0a864693d45969a0c1936c\n",
            "Successfully built jsonnet overrides ftfy numpydoc parsimonious jsondiff\n",
            "\u001b[31mfeaturetools 0.4.1 has requirement pandas>=0.23.0, but you'll have pandas 0.22.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mcufflinks 0.14.6 has requirement plotly>=3.0.0, but you'll have plotly 1.12.12 which is incompatible.\u001b[0m\n",
            "\u001b[31mawscli 1.16.92 has requirement botocore==1.12.82, but you'll have botocore 1.12.78 which is incompatible.\u001b[0m\n",
            "Installing collected packages: responses, torch, ecdsa, pycryptodome, python-jose, docker-pycreds, websocket-client, docker, jsondiff, jsonpickle, aws-xray-sdk, xmltodict, pbr, mock, pyaml, pytz, cookies, asn1crypto, cryptography, moto, jsonnet, pytorch-pretrained-bert, overrides, ftfy, rsa, colorama, awscli, matplotlib, tensorboardX, numpydoc, gevent, conllu, parsimonious, flask-cors, flaky, allennlp\n",
            "  Found existing installation: pytz 2018.9\n",
            "    Uninstalling pytz-2018.9:\n",
            "      Successfully uninstalled pytz-2018.9\n",
            "  Found existing installation: rsa 4.0\n",
            "    Uninstalling rsa-4.0:\n",
            "      Successfully uninstalled rsa-4.0\n",
            "  Found existing installation: matplotlib 3.0.2\n",
            "    Uninstalling matplotlib-3.0.2:\n",
            "      Successfully uninstalled matplotlib-3.0.2\n",
            "  Found existing installation: gevent 1.4.0\n",
            "    Uninstalling gevent-1.4.0:\n",
            "      Successfully uninstalled gevent-1.4.0\n",
            "Successfully installed allennlp-0.8.1 asn1crypto-0.24.0 aws-xray-sdk-0.95 awscli-1.16.92 colorama-0.3.9 conllu-0.11 cookies-2.2.1 cryptography-2.4.2 docker-3.7.0 docker-pycreds-0.4.0 ecdsa-0.13 flaky-3.5.3 flask-cors-3.0.7 ftfy-5.5.0 gevent-1.3.6 jsondiff-1.1.1 jsonnet-0.10.0 jsonpickle-1.0 matplotlib-2.2.3 mock-2.0.0 moto-1.3.4 numpydoc-0.8.0 overrides-1.9 parsimonious-0.8.0 pbr-5.1.1 pyaml-18.11.0 pycryptodome-3.7.2 python-jose-2.0.2 pytorch-pretrained-bert-0.3.0 pytz-2017.3 responses-0.10.5 rsa-3.4.2 tensorboardX-1.2 torch-1.0.0 websocket-client-0.54.0 xmltodict-0.11.0\n",
            "\u001b[0;31;1mWARNING: The following packages were previously imported in this runtime:\n",
            "  [matplotlib, mpl_toolkits, pytz, rsa]\n",
            "You must restart the runtime in order to use newly installed versions.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DCq5Ib35X8Tb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "484a284f-6a3e-4ae0-cd62-ed781345269a"
      },
      "cell_type": "code",
      "source": [
        "!mv  ./realworldnlp/realworldnlp/predictors.py ./realworldnlp\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mv: cannot stat './realworldnlp/realworldnlp/predictors.py': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1oOoYTtCYFHd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "054b131e-14b8-4d54-9e41-0d69a6ee8ddc"
      },
      "cell_type": "code",
      "source": [
        "!unzip trainDevTestTrees_PTB.zip"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  trainDevTestTrees_PTB.zip\n",
            "   creating: trees/\n",
            "  inflating: trees/dev.txt           \n",
            "  inflating: trees/test.txt          \n",
            "  inflating: trees/train.txt         \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jnH3z8Ms9mAI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from typing import Dict\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from allennlp.data.dataset_readers.stanford_sentiment_tree_bank import \\\n",
        "    StanfordSentimentTreeBankDatasetReader\n",
        "from allennlp.data.iterators import BucketIterator\n",
        "from allennlp.data.vocabulary import Vocabulary\n",
        "from allennlp.models import Model\n",
        "from allennlp.modules.seq2vec_encoders import Seq2VecEncoder, PytorchSeq2VecWrapper\n",
        "from allennlp.modules.text_field_embedders import TextFieldEmbedder, BasicTextFieldEmbedder\n",
        "from allennlp.modules.token_embedders import Embedding\n",
        "from allennlp.nn.util import get_text_field_mask\n",
        "from allennlp.training.metrics import CategoricalAccuracy\n",
        "from allennlp.training.trainer import Trainer\n",
        "\n",
        "from realworldnlp.predictors import SentenceClassifierPredictor\n",
        "\n",
        "EMBEDDING_DIM = 128\n",
        "HIDDEN_DIM = 128\n",
        "\n",
        "# Model in AllenNLP represents a model that is trained.\n",
        "@Model.register(\"lstm_classify\") #use a name different from class for the model\n",
        "class LstmClassifier(Model):\n",
        "    def __init__(self,\n",
        "                 word_embeddings: TextFieldEmbedder,\n",
        "                 encoder: Seq2VecEncoder,\n",
        "                 vocab: Vocabulary) -> None:\n",
        "        super().__init__(vocab)\n",
        "        # We need the embeddings to convert word IDs to their vector representations\n",
        "        self.word_embeddings = word_embeddings\n",
        "\n",
        "        # Seq2VecEncoder is a neural network abstraction that takes a sequence of something\n",
        "        # (usually a sequence of embedded word vectors), processes it, and returns a single\n",
        "        # vector. Oftentimes this is an RNN-based architecture (e.g., LSTM or GRU), but\n",
        "        # AllenNLP also supports CNNs and other simple architectures (for example,\n",
        "        # just averaging over the input vectors).\n",
        "        self.encoder = encoder\n",
        "\n",
        "        # After converting a sequence of vectors to a single vector, we feed it into\n",
        "        # a fully-connected linear layer to reduce the dimension to the total number of labels.\n",
        "        self.hidden2tag = torch.nn.Linear(in_features=encoder.get_output_dim(),\n",
        "                                          out_features=vocab.get_vocab_size('labels'))\n",
        "        self.accuracy = CategoricalAccuracy()\n",
        "\n",
        "        # We use the cross entropy loss because this is a classification task.\n",
        "        # Note that PyTorch's CrossEntropyLoss combines softmax and log likelihood loss,\n",
        "        # which makes it unnecessary to add a separate softmax layer.\n",
        "        self.loss_function = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    # Instances are fed to forward after batching.\n",
        "    # Fields are passed through arguments with the same name.\n",
        "    def forward(self,\n",
        "                tokens: Dict[str, torch.Tensor],\n",
        "                label: torch.Tensor = None) -> torch.Tensor:\n",
        "        # In deep NLP, when sequences of tensors in different lengths are batched together,\n",
        "        # shorter sequences get padded with zeros to make them equal length.\n",
        "        # Masking is the process to ignore extra zeros added by padding\n",
        "        mask = get_text_field_mask(tokens)\n",
        "\n",
        "        # Forward pass\n",
        "        embeddings = self.word_embeddings(tokens)\n",
        "        encoder_out = self.encoder(embeddings, mask)\n",
        "        logits = self.hidden2tag(encoder_out)\n",
        "\n",
        "        # In AllenNLP, the output of forward() is a dictionary.\n",
        "        # Your output dictionary must contain a \"loss\" key for your model to be trained.\n",
        "        output = {\"logits\": logits}\n",
        "        if label is not None:\n",
        "            self.accuracy(logits, label)\n",
        "            output[\"loss\"] = self.loss_function(logits, label)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n",
        "        return {\"accuracy\": self.accuracy.get_metric(reset)}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Rcpuh1XLBpxy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# PATH = \"./model.pth\"\n",
        "def main():\n",
        "    reader = StanfordSentimentTreeBankDatasetReader()\n",
        "\n",
        "    train_dataset = reader.read('./trees/train.txt')\n",
        "    dev_dataset = reader.read('./trees/dev.txt')\n",
        "    \n",
        "    # You can optionally specify the minimum count of tokens/labels.\n",
        "    # `min_count={'tokens':3}` here means that any tokens that appear less than three times\n",
        "    # will be ignored and not included in the vocabulary.\n",
        "    vocab = Vocabulary.from_instances(train_dataset + dev_dataset,\n",
        "                                      min_count={'tokens': 3})\n",
        "\n",
        "    token_embedding = Embedding(num_embeddings=vocab.get_vocab_size('tokens'),\n",
        "                                embedding_dim=EMBEDDING_DIM)\n",
        "\n",
        "    # BasicTextFieldEmbedder takes a dict - we need an embedding just for tokens,\n",
        "    # not for labels, which are used as-is as the \"answer\" of the sentence classification\n",
        "    word_embeddings = BasicTextFieldEmbedder({\"tokens\": token_embedding})\n",
        "\n",
        "    lstm = PytorchSeq2VecWrapper(\n",
        "        torch.nn.LSTM(EMBEDDING_DIM, HIDDEN_DIM, batch_first=True))\n",
        "\n",
        "    model = LstmClassifier(word_embeddings, lstm, vocab)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "\n",
        "    iterator = BucketIterator(batch_size=32, sorting_keys=[(\"tokens\", \"num_tokens\")])\n",
        "\n",
        "    iterator.index_with(vocab)\n",
        "\n",
        "    trainer = Trainer(model=model,\n",
        "                      optimizer=optimizer,\n",
        "                      iterator=iterator,\n",
        "                      train_dataset=train_dataset,\n",
        "                      validation_dataset=dev_dataset,\n",
        "                      num_epochs=10)\n",
        "\n",
        "    trainer.train()\n",
        "#     torch.save(model, PATH)\n",
        "    tokens = ['This', 'is', 'the', 'worst', 'movie', 'ever', '!']\n",
        "# model = torch.load(PATH)\n",
        "# model.eval()\n",
        "\n",
        "    predictor = SentenceClassifierPredictor(model, dataset_reader=reader)\n",
        "    logits = predictor.predict(tokens)['logits']\n",
        "    label_id = np.argmax(logits)\n",
        "    \n",
        "    print(\"0 - very negative\\n 1 - negative\\n 2 - neutral\\n 3 - positive\\n 4 - very positive\\n\")\n",
        "    print(\"Sentiment level: {}\".format(model.vocab.get_token_from_index(label_id, 'labels')))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8v1P3m_1FO7k",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# tokens = ['This', 'is', 'the', 'best', 'movie', 'ever', '!']\n",
        "# # model = torch.load(PATH)\n",
        "# # model.eval()\n",
        "\n",
        "# # reader = StanfordSentimentTreeBankDatasetReader()\n",
        "# # train_dataset = reader.read('./trees/train.txt')\n",
        "# # dev_dataset = reader.read('./trees/dev.txt')\n",
        "  \n",
        "# predictor = SentenceClassifierPredictor(model, dataset_reader=reader)\n",
        "# logits = predictor.predict(tokens)['logits']\n",
        "# label_id = np.argmax(logits)\n",
        "\n",
        "# print(model.vocab.get_token_from_index(label_id, 'labels'))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RjKdTcBJ9pc6",
        "colab_type": "code",
        "outputId": "4ba9e585-2e51-4a48-b214-a45d4976a4a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2707
        }
      },
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0it [00:00, ?it/s]01/20/2019 08:36:22 - INFO - allennlp.data.dataset_readers.stanford_sentiment_tree_bank -   Reading instances from lines in file at: ./trees/train.txt\n",
            "8544it [00:02, 3950.68it/s]\n",
            "0it [00:00, ?it/s]01/20/2019 08:36:24 - INFO - allennlp.data.dataset_readers.stanford_sentiment_tree_bank -   Reading instances from lines in file at: ./trees/dev.txt\n",
            "1101it [00:00, 4771.91it/s]\n",
            "01/20/2019 08:36:24 - INFO - allennlp.data.vocabulary -   Fitting token dictionary from dataset.\n",
            "100%|██████████| 9645/9645 [00:00<00:00, 42473.15it/s]\n",
            "01/20/2019 08:36:24 - WARNING - allennlp.training.trainer -   You provided a validation dataset but patience was set to None, meaning that early stopping is disabled\n",
            "01/20/2019 08:36:24 - INFO - allennlp.training.trainer -   Beginning training.\n",
            "01/20/2019 08:36:24 - INFO - allennlp.training.trainer -   Epoch 0/9\n",
            "01/20/2019 08:36:24 - INFO - allennlp.training.trainer -   Peak CPU memory usage MB: 440.984\n",
            "01/20/2019 08:36:24 - INFO - allennlp.training.trainer -   GPU 0 memory usage MB: 11\n",
            "01/20/2019 08:36:24 - INFO - allennlp.training.trainer -   Training\n",
            "accuracy: 0.2639, loss: 1.5836 ||: 100%|██████████| 267/267 [00:13<00:00, 21.55it/s]\n",
            "01/20/2019 08:36:38 - INFO - allennlp.training.trainer -   Validating\n",
            "accuracy: 0.2625, loss: 1.5735 ||: 100%|██████████| 35/35 [00:00<00:00, 77.93it/s]\n",
            "01/20/2019 08:36:39 - INFO - allennlp.training.trainer -                       Training |  Validation\n",
            "01/20/2019 08:36:39 - INFO - allennlp.training.trainer -   loss            |     1.584  |     1.573\n",
            "01/20/2019 08:36:39 - INFO - allennlp.training.trainer -   accuracy        |     0.264  |     0.262\n",
            "01/20/2019 08:36:39 - INFO - allennlp.training.trainer -   cpu_memory_MB   |   440.984  |       N/A\n",
            "01/20/2019 08:36:39 - INFO - allennlp.training.trainer -   gpu_0_memory_MB |    11.000  |       N/A\n",
            "01/20/2019 08:36:39 - INFO - allennlp.training.trainer -   Epoch duration: 00:00:14\n",
            "01/20/2019 08:36:39 - INFO - allennlp.training.trainer -   Estimated training time remaining: 0:02:09\n",
            "01/20/2019 08:36:39 - INFO - allennlp.training.trainer -   Epoch 1/9\n",
            "01/20/2019 08:36:39 - INFO - allennlp.training.trainer -   Peak CPU memory usage MB: 497.492\n",
            "01/20/2019 08:36:39 - INFO - allennlp.training.trainer -   GPU 0 memory usage MB: 11\n",
            "01/20/2019 08:36:39 - INFO - allennlp.training.trainer -   Training\n",
            "accuracy: 0.2783, loss: 1.5652 ||: 100%|██████████| 267/267 [00:13<00:00, 20.24it/s]\n",
            "01/20/2019 08:36:52 - INFO - allennlp.training.trainer -   Validating\n",
            "accuracy: 0.2616, loss: 1.5737 ||: 100%|██████████| 35/35 [00:00<00:00, 90.73it/s]\n",
            "01/20/2019 08:36:52 - INFO - allennlp.training.trainer -                       Training |  Validation\n",
            "01/20/2019 08:36:52 - INFO - allennlp.training.trainer -   loss            |     1.565  |     1.574\n",
            "01/20/2019 08:36:52 - INFO - allennlp.training.trainer -   accuracy        |     0.278  |     0.262\n",
            "01/20/2019 08:36:52 - INFO - allennlp.training.trainer -   cpu_memory_MB   |   497.492  |       N/A\n",
            "01/20/2019 08:36:52 - INFO - allennlp.training.trainer -   gpu_0_memory_MB |    11.000  |       N/A\n",
            "01/20/2019 08:36:52 - INFO - allennlp.training.trainer -   Epoch duration: 00:00:13\n",
            "01/20/2019 08:36:52 - INFO - allennlp.training.trainer -   Estimated training time remaining: 0:01:52\n",
            "01/20/2019 08:36:52 - INFO - allennlp.training.trainer -   Epoch 2/9\n",
            "01/20/2019 08:36:52 - INFO - allennlp.training.trainer -   Peak CPU memory usage MB: 503.076\n",
            "01/20/2019 08:36:52 - INFO - allennlp.training.trainer -   GPU 0 memory usage MB: 11\n",
            "01/20/2019 08:36:52 - INFO - allennlp.training.trainer -   Training\n",
            "accuracy: 0.2773, loss: 1.5559 ||: 100%|██████████| 267/267 [00:13<00:00, 17.34it/s]\n",
            "01/20/2019 08:37:06 - INFO - allennlp.training.trainer -   Validating\n",
            "accuracy: 0.2734, loss: 1.5697 ||: 100%|██████████| 35/35 [00:00<00:00, 89.11it/s]\n",
            "01/20/2019 08:37:06 - INFO - allennlp.training.trainer -                       Training |  Validation\n",
            "01/20/2019 08:37:06 - INFO - allennlp.training.trainer -   loss            |     1.556  |     1.570\n",
            "01/20/2019 08:37:06 - INFO - allennlp.training.trainer -   accuracy        |     0.277  |     0.273\n",
            "01/20/2019 08:37:06 - INFO - allennlp.training.trainer -   cpu_memory_MB   |   503.076  |       N/A\n",
            "01/20/2019 08:37:06 - INFO - allennlp.training.trainer -   gpu_0_memory_MB |    11.000  |       N/A\n",
            "01/20/2019 08:37:06 - INFO - allennlp.training.trainer -   Epoch duration: 00:00:13\n",
            "01/20/2019 08:37:06 - INFO - allennlp.training.trainer -   Estimated training time remaining: 0:01:37\n",
            "01/20/2019 08:37:06 - INFO - allennlp.training.trainer -   Epoch 3/9\n",
            "01/20/2019 08:37:06 - INFO - allennlp.training.trainer -   Peak CPU memory usage MB: 504.772\n",
            "01/20/2019 08:37:06 - INFO - allennlp.training.trainer -   GPU 0 memory usage MB: 11\n",
            "01/20/2019 08:37:06 - INFO - allennlp.training.trainer -   Training\n",
            "accuracy: 0.2917, loss: 1.5306 ||: 100%|██████████| 267/267 [00:13<00:00, 20.12it/s]\n",
            "01/20/2019 08:37:19 - INFO - allennlp.training.trainer -   Validating\n",
            "accuracy: 0.2761, loss: 1.5604 ||: 100%|██████████| 35/35 [00:00<00:00, 88.81it/s]\n",
            "01/20/2019 08:37:20 - INFO - allennlp.training.trainer -                       Training |  Validation\n",
            "01/20/2019 08:37:20 - INFO - allennlp.training.trainer -   loss            |     1.531  |     1.560\n",
            "01/20/2019 08:37:20 - INFO - allennlp.training.trainer -   accuracy        |     0.292  |     0.276\n",
            "01/20/2019 08:37:20 - INFO - allennlp.training.trainer -   cpu_memory_MB   |   504.772  |       N/A\n",
            "01/20/2019 08:37:20 - INFO - allennlp.training.trainer -   gpu_0_memory_MB |    11.000  |       N/A\n",
            "01/20/2019 08:37:20 - INFO - allennlp.training.trainer -   Epoch duration: 00:00:13\n",
            "01/20/2019 08:37:20 - INFO - allennlp.training.trainer -   Estimated training time remaining: 0:01:23\n",
            "01/20/2019 08:37:20 - INFO - allennlp.training.trainer -   Epoch 4/9\n",
            "01/20/2019 08:37:20 - INFO - allennlp.training.trainer -   Peak CPU memory usage MB: 506.624\n",
            "01/20/2019 08:37:20 - INFO - allennlp.training.trainer -   GPU 0 memory usage MB: 11\n",
            "01/20/2019 08:37:20 - INFO - allennlp.training.trainer -   Training\n",
            "accuracy: 0.3323, loss: 1.4648 ||: 100%|██████████| 267/267 [00:13<00:00, 20.37it/s]\n",
            "01/20/2019 08:37:33 - INFO - allennlp.training.trainer -   Validating\n",
            "accuracy: 0.3351, loss: 1.5264 ||: 100%|██████████| 35/35 [00:00<00:00, 87.79it/s]\n",
            "01/20/2019 08:37:33 - INFO - allennlp.training.trainer -                       Training |  Validation\n",
            "01/20/2019 08:37:33 - INFO - allennlp.training.trainer -   loss            |     1.465  |     1.526\n",
            "01/20/2019 08:37:33 - INFO - allennlp.training.trainer -   accuracy        |     0.332  |     0.335\n",
            "01/20/2019 08:37:33 - INFO - allennlp.training.trainer -   cpu_memory_MB   |   506.624  |       N/A\n",
            "01/20/2019 08:37:33 - INFO - allennlp.training.trainer -   gpu_0_memory_MB |    11.000  |       N/A\n",
            "01/20/2019 08:37:33 - INFO - allennlp.training.trainer -   Epoch duration: 00:00:13\n",
            "01/20/2019 08:37:33 - INFO - allennlp.training.trainer -   Estimated training time remaining: 0:01:09\n",
            "01/20/2019 08:37:33 - INFO - allennlp.training.trainer -   Epoch 5/9\n",
            "01/20/2019 08:37:33 - INFO - allennlp.training.trainer -   Peak CPU memory usage MB: 507.636\n",
            "01/20/2019 08:37:33 - INFO - allennlp.training.trainer -   GPU 0 memory usage MB: 11\n",
            "01/20/2019 08:37:33 - INFO - allennlp.training.trainer -   Training\n",
            "accuracy: 0.4325, loss: 1.3398 ||: 100%|██████████| 267/267 [00:13<00:00, 20.21it/s]\n",
            "01/20/2019 08:37:47 - INFO - allennlp.training.trainer -   Validating\n",
            "accuracy: 0.3488, loss: 1.4945 ||: 100%|██████████| 35/35 [00:00<00:00, 87.48it/s]\n",
            "01/20/2019 08:37:47 - INFO - allennlp.training.trainer -                       Training |  Validation\n",
            "01/20/2019 08:37:47 - INFO - allennlp.training.trainer -   loss            |     1.340  |     1.495\n",
            "01/20/2019 08:37:47 - INFO - allennlp.training.trainer -   accuracy        |     0.432  |     0.349\n",
            "01/20/2019 08:37:47 - INFO - allennlp.training.trainer -   cpu_memory_MB   |   507.636  |       N/A\n",
            "01/20/2019 08:37:47 - INFO - allennlp.training.trainer -   gpu_0_memory_MB |    11.000  |       N/A\n",
            "01/20/2019 08:37:47 - INFO - allennlp.training.trainer -   Epoch duration: 00:00:13\n",
            "01/20/2019 08:37:47 - INFO - allennlp.training.trainer -   Estimated training time remaining: 0:00:55\n",
            "01/20/2019 08:37:47 - INFO - allennlp.training.trainer -   Epoch 6/9\n",
            "01/20/2019 08:37:47 - INFO - allennlp.training.trainer -   Peak CPU memory usage MB: 510.396\n",
            "01/20/2019 08:37:47 - INFO - allennlp.training.trainer -   GPU 0 memory usage MB: 11\n",
            "01/20/2019 08:37:47 - INFO - allennlp.training.trainer -   Training\n",
            "accuracy: 0.5142, loss: 1.1934 ||: 100%|██████████| 267/267 [00:13<00:00, 18.67it/s]\n",
            "01/20/2019 08:38:00 - INFO - allennlp.training.trainer -   Validating\n",
            "accuracy: 0.3697, loss: 1.5018 ||: 100%|██████████| 35/35 [00:00<00:00, 89.19it/s]\n",
            "01/20/2019 08:38:01 - INFO - allennlp.training.trainer -                       Training |  Validation\n",
            "01/20/2019 08:38:01 - INFO - allennlp.training.trainer -   loss            |     1.193  |     1.502\n",
            "01/20/2019 08:38:01 - INFO - allennlp.training.trainer -   accuracy        |     0.514  |     0.370\n",
            "01/20/2019 08:38:01 - INFO - allennlp.training.trainer -   cpu_memory_MB   |   510.396  |       N/A\n",
            "01/20/2019 08:38:01 - INFO - allennlp.training.trainer -   gpu_0_memory_MB |    11.000  |       N/A\n",
            "01/20/2019 08:38:01 - INFO - allennlp.training.trainer -   Epoch duration: 00:00:13\n",
            "01/20/2019 08:38:01 - INFO - allennlp.training.trainer -   Estimated training time remaining: 0:00:41\n",
            "01/20/2019 08:38:01 - INFO - allennlp.training.trainer -   Epoch 7/9\n",
            "01/20/2019 08:38:01 - INFO - allennlp.training.trainer -   Peak CPU memory usage MB: 510.428\n",
            "01/20/2019 08:38:01 - INFO - allennlp.training.trainer -   GPU 0 memory usage MB: 11\n",
            "01/20/2019 08:38:01 - INFO - allennlp.training.trainer -   Training\n",
            "accuracy: 0.5724, loss: 1.0563 ||: 100%|██████████| 267/267 [00:13<00:00, 19.95it/s]\n",
            "01/20/2019 08:38:14 - INFO - allennlp.training.trainer -   Validating\n",
            "accuracy: 0.3778, loss: 1.5125 ||: 100%|██████████| 35/35 [00:00<00:00, 88.60it/s]\n",
            "01/20/2019 08:38:15 - INFO - allennlp.training.trainer -                       Training |  Validation\n",
            "01/20/2019 08:38:15 - INFO - allennlp.training.trainer -   loss            |     1.056  |     1.512\n",
            "01/20/2019 08:38:15 - INFO - allennlp.training.trainer -   accuracy        |     0.572  |     0.378\n",
            "01/20/2019 08:38:15 - INFO - allennlp.training.trainer -   cpu_memory_MB   |   510.428  |       N/A\n",
            "01/20/2019 08:38:15 - INFO - allennlp.training.trainer -   gpu_0_memory_MB |    11.000  |       N/A\n",
            "01/20/2019 08:38:15 - INFO - allennlp.training.trainer -   Epoch duration: 00:00:13\n",
            "01/20/2019 08:38:15 - INFO - allennlp.training.trainer -   Estimated training time remaining: 0:00:27\n",
            "01/20/2019 08:38:15 - INFO - allennlp.training.trainer -   Epoch 8/9\n",
            "01/20/2019 08:38:15 - INFO - allennlp.training.trainer -   Peak CPU memory usage MB: 511.4\n",
            "01/20/2019 08:38:15 - INFO - allennlp.training.trainer -   GPU 0 memory usage MB: 11\n",
            "01/20/2019 08:38:15 - INFO - allennlp.training.trainer -   Training\n",
            "accuracy: 0.6097, loss: 0.9596 ||: 100%|██████████| 267/267 [00:13<00:00, 22.58it/s]\n",
            "01/20/2019 08:38:28 - INFO - allennlp.training.trainer -   Validating\n",
            "accuracy: 0.3551, loss: 1.5786 ||: 100%|██████████| 35/35 [00:00<00:00, 88.09it/s]\n",
            "01/20/2019 08:38:28 - INFO - allennlp.training.trainer -                       Training |  Validation\n",
            "01/20/2019 08:38:28 - INFO - allennlp.training.trainer -   loss            |     0.960  |     1.579\n",
            "01/20/2019 08:38:28 - INFO - allennlp.training.trainer -   accuracy        |     0.610  |     0.355\n",
            "01/20/2019 08:38:28 - INFO - allennlp.training.trainer -   cpu_memory_MB   |   511.400  |       N/A\n",
            "01/20/2019 08:38:28 - INFO - allennlp.training.trainer -   gpu_0_memory_MB |    11.000  |       N/A\n",
            "01/20/2019 08:38:28 - INFO - allennlp.training.trainer -   Epoch duration: 00:00:13\n",
            "01/20/2019 08:38:28 - INFO - allennlp.training.trainer -   Estimated training time remaining: 0:00:13\n",
            "01/20/2019 08:38:28 - INFO - allennlp.training.trainer -   Epoch 9/9\n",
            "01/20/2019 08:38:28 - INFO - allennlp.training.trainer -   Peak CPU memory usage MB: 512.776\n",
            "01/20/2019 08:38:28 - INFO - allennlp.training.trainer -   GPU 0 memory usage MB: 11\n",
            "01/20/2019 08:38:28 - INFO - allennlp.training.trainer -   Training\n",
            "accuracy: 0.6424, loss: 0.8754 ||: 100%|██████████| 267/267 [00:13<00:00, 20.16it/s]\n",
            "01/20/2019 08:38:42 - INFO - allennlp.training.trainer -   Validating\n",
            "accuracy: 0.3606, loss: 1.6384 ||: 100%|██████████| 35/35 [00:00<00:00, 86.97it/s]\n",
            "01/20/2019 08:38:42 - INFO - allennlp.training.trainer -                       Training |  Validation\n",
            "01/20/2019 08:38:42 - INFO - allennlp.training.trainer -   loss            |     0.875  |     1.638\n",
            "01/20/2019 08:38:42 - INFO - allennlp.training.trainer -   accuracy        |     0.642  |     0.361\n",
            "01/20/2019 08:38:42 - INFO - allennlp.training.trainer -   cpu_memory_MB   |   512.776  |       N/A\n",
            "01/20/2019 08:38:42 - INFO - allennlp.training.trainer -   gpu_0_memory_MB |    11.000  |       N/A\n",
            "01/20/2019 08:38:42 - INFO - allennlp.training.trainer -   Epoch duration: 00:00:13\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0 - very negative\n",
            " 1 - negative\n",
            " 2 - neutral\n",
            " 3 - positive\n",
            " 4 - very positive\n",
            "\n",
            "Sentiment level: 0\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}